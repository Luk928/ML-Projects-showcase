{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Łukasz Ozimek 236529\n",
    "### Laboratorium 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD SUMMARIES FROM PCA DOWN\n",
    "\n",
    "<a id='part_1_contents'></a>\n",
    "Following the teacher's recommendations I documented only algorithms that are a part of scikit-learn library. I omitted parameters, attributes and methods which are being deprecated as of version 0.23. The documented algorithms:\n",
    "\n",
    "* [Ordinary Least Squares Regression (Linear Regression)](#lin_reg_cell)\n",
    "* [Logistic Regression](#log_reg_cell)\n",
    "* [K-Nearest Neighbors](#knn_cell)\n",
    "* [Ridge Regression](#ridge-reg_cell)\n",
    "* [LASSO](#lasso_cell)\n",
    "* [Elastic Net](#elastic_net_cell)\n",
    "* [LARS](#lars_cell)\n",
    "* [Decision Tree](#tree_cell)\n",
    "* [Naive Bayes](#nb_cell)\n",
    "* [Gaussian Naive Bayes](#gnb_cell)\n",
    "* [Multinomial Naive Bayes](#mnb_cell)\n",
    "* [K-means](#k_means_cell)\n",
    "* [Hierarchical Clustering](#HC_cell)\n",
    "* [Perceptron](#perceptron_cell)\n",
    "* [Back Propagation (Multilayer Percpetron)](#back_cell)\n",
    "* [Deep Boltzmann Machine](#dbm_cell)\n",
    "* [Principal Component Analysis](#pca_cell)\n",
    "* [Partial Least Squares Regression](#plsr_cell)\n",
    "* [Multidimensional Scaling](#ms_cell)\n",
    "* [Linear Discriminant Analysis](#lda_cell)\n",
    "* [Quadratic Discriminant Analysis](#qda_cell)\n",
    "* [Bootstrapped Aggregation (Bagging)](#BA_cell)\n",
    "* [Stacked Generalization](#SG_cell)\n",
    "* [Gradient Boosting Machines](#GBM_cell)\n",
    "* [AdaBoost](#Ada_cell)\n",
    "* [Random Forest](#RF_cell)\n",
    "\n",
    "<br>\n",
    "Algorithms listed in the laboratory instruction but not being a part of scikit-learn (I looked fot them in both 0.23 and 0.16 versions of scikit-learn):\n",
    "\n",
    "* Stepwise Regression\n",
    "* Multivariate Adaptive Regression Splines\n",
    "* Locally Estimated Scatterplot Smoothing \n",
    "* Learning Vector Quantization\n",
    "* Self-Organizing Map\n",
    "* Locally Weighted Learning\n",
    "* Iterative Dichotomiser 3\n",
    "* C4.5 and C5.0\n",
    "* Chi-squared Automatic Interaction Detection\n",
    "* Decision stump\n",
    "* M5\n",
    "* Conditional Decision Trees\n",
    "* Averaged One-Dependence Estimator\n",
    "* Bayesian Belief Network\n",
    "* Bayesian Network\n",
    "* K-Medians\n",
    "* Expectation Maximisation\n",
    "* Apriori Algorithm\n",
    "* Eclat Algorithm\n",
    "* Hopfield Network\n",
    "* Radial Basis Function Network\n",
    "* Deep Belief Networks \n",
    "* Principal Component Regression\n",
    "* Sammon Mapping\n",
    "* Projection Pursuit\n",
    "* Mixture Discriminant Analysis - MDA\n",
    "* Flexible Discriminant Analysis - FDA\n",
    "* Drzewa regresji wzmacniane gradientem (Gradient Boosted regression Trees - GBRT)\n",
    "- Convolutional Neural Network (CNN)\n",
    "- Stacked Auto-Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lin_reg_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)\n",
    "\n",
    "\"\"\"\n",
    "Summary:\n",
    "A supervised learning model aiming to predict a value based on the features. It is computationally efficient, \n",
    "easy to interpret and very simple, however it's simplicty makes it unable to be efficiently used on complicated \n",
    "and non-linear data.\n",
    "\n",
    "Parameters:\n",
    "fit_intercept = Whether to calculate the intercept for the model\n",
    "normalize = Normlizing inputs before training by subtracting the mean and dividing by the l2-norm.\n",
    "copy_X = Whether X should be copied or overwritten\n",
    "n_jobs = Number of used processors\n",
    "\n",
    "Attributes:\n",
    "coef_ = Estimated coefficients for the linear regression problem.\n",
    "rank_ = Rank of matrix X. Only available when X is dense.\n",
    "singular_ = Singular values of X. Only available when X is dense.\n",
    "intercept_ = Independent term in the linear model. Set to 0.0 if fit_intercept = False.\n",
    "\n",
    "Methods:\n",
    "fit(X, y[, sample_weight]) = Fit linear model.\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "predict(X) = Predict using the linear model.\n",
    "score(X, y[, sample_weight]) = Return the coefficient of determination R^2 of the prediction.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "\"\"\"\n",
    "print() # Needed to supress the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='log_reg_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True,\n",
    "                         intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs',\n",
    "                         max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "\n",
    "\"\"\"\n",
    "Summary:\n",
    "A supervised learning model aiming to classify samples into classes. Just like linear regression it's one of the \n",
    "simples classification algorithms, however it makes it inefficient when dealing with complex data.\n",
    "\n",
    "Parameters:\n",
    "penalty{‘l1’, ‘l2’, ‘elasticnet’, ‘none’} = Used to specify the norm used in the penalization.\n",
    "dualbool = Dual (only for l2 penalty with liblinear solver) or primal formulation. Prefer dual=False when n_samples > n_features.\n",
    "tolfloat = Tolerance for stopping criteria.\n",
    "Cfloat = Inverse of regularization strength; must be a positive float. Smaller values specify stronger regularization.\n",
    "fit_intercept =  whether to calculate the intercept for the model\n",
    "intercept_scaling = (Only with solver=‘liblinear’ and fit_intercept=True) X becomes a “synthetic” feature with constant \n",
    "    value equal to intercept_scaling is appended to the instance vector. \n",
    "class_weight = Weights associated with classes in the form {class_label: weight}.\n",
    "random_state = Only when solver=‘sag’, ‘saga’ or ‘liblinear’. Shuffle the data.\n",
    "solver{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’} = Algorithm to use in the optimization problem.\n",
    "    For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones.\n",
    "    For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss; ‘liblinear’ is limited to one-versus-rest schemes.\n",
    "    ‘newton-cg’, ‘lbfgs’, ‘sag’ and ‘saga’ handle L2 or no penalty\n",
    "    ‘liblinear’ and ‘saga’ also handle L1 penalty\n",
    "    ‘saga’ also supports ‘elasticnet’ penalty\n",
    "    ‘liblinear’ does not support setting penalty='none'\n",
    "    Note that ‘sag’ and ‘saga’ fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.\n",
    "max_iterint = Maximum number of iterations taken for the solvers to converge.\n",
    "multi_class{‘auto’, ‘ovr’, ‘multinomial’} = If the option chosen is ‘ovr’, then a binary problem is fit for each label.\n",
    "    For ‘multinomial’ the loss minimised is the multinomial loss fit across the entire probability distribution, \n",
    "    even when the data is binary. ‘multinomial’ is unavailable when solver=’liblinear’. \n",
    "    ‘auto’ selects ‘ovr’ if the data is binary, or if solver=’liblinear’, and otherwise selects ‘multinomial’.\n",
    "verboseint = Set verbose to any positive number for verbosity.\n",
    "warm_start = Reuse the solution of the previous call to fit as initialization, or erase the previous solution. \n",
    "    Useless for liblinear solver.\n",
    "n_jobs = Number of used processors\n",
    "l1_ratio = The Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1. Only used if penalty='elasticnet'. \n",
    "Setting l1_ratio=0 is equivalent to using penalty='l2', while setting l1_ratio=1 is equivalent to using penalty='l1'. \n",
    "For 0 < l1_ratio <1, the penalty is a combination of L1 and L2.\n",
    "\n",
    "Attributes:\n",
    "classes_ = A list of class labels known to the classifier.\n",
    "coef_ = Estimated coefficients for the logistic regression problem\n",
    "intercept_ = Independent term in the linear model. Set to 0.0 if fit_intercept = False.\n",
    "n_iter_ = Actual number of iterations for all classes. \n",
    "\n",
    "Methods:\n",
    "decision_function(X) = Predict confidence scores for samples.\n",
    "densify() = Convert coefficient matrix to dense array format.\n",
    "fit(X, y[, sample_weight]) = Fit the model according to the given training data.\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "predict(X) = Predict class labels for samples in X.\n",
    "predict_log_proba(X) = Predict logarithm of probability estimates.\n",
    "predict_proba(X) = Probability estimates.\n",
    "score(X, y[, sample_weight]) = Return the mean accuracy on the given test data and labels.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "sparsify() = Convert coefficient matrix to sparse format.\n",
    "\"\"\"\n",
    "print() # Needed to supress the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='knn_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "model = KNeighborsRegressor(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
    "\"\"\"\n",
    "Summary:\n",
    "A supervised learning model aiming to as either a classifier or a regressor. It predicts the values based on the sample's \n",
    "proximity to other samples.It's quick and simple however it can be slow, needs homogenous data, is sensitive to outliers and \n",
    "requires balanced data (similar amount of samples for each class).\n",
    "\n",
    "Parameters:\n",
    "n_neighbors = Number of neighbors to use by default for kneighbors queries.\n",
    "weights{‘uniform’, ‘distance’}  = Weight function used in prediction. Possible values:\n",
    "    ‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.\n",
    "    ‘distance’ : weight points by the inverse of their distance. in this case, \n",
    "    closer neighbors of a query point will have a greater influence than neighbors which are further away.\n",
    "    [callable] : a user-defined function which accepts an array of distances, and returns an array of \n",
    "    the same shape containing the weights.\n",
    "algorithm{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’} = Algorithm used to compute the nearest neighbors.\n",
    "leaf_sizeint = Leaf size passed to BallTree or KDTree.\n",
    "pint = Power parameter for the Minkowski metric.\n",
    "metric = The distance metric to use for the tree. Metrics:\n",
    "    “euclidean”\n",
    "    “manhattan”\n",
    "    “chebyshev”\n",
    "    “minkowski”\n",
    "    “wminkowski”\n",
    "    “seuclidean”\n",
    "    “mahalanobis”\n",
    "metric_params = Additional keyword arguments for the metric function.\n",
    "n_jobs = Number of used processors\n",
    "\n",
    "Attributes:\n",
    "effective_metric_ = The distance metric used.\n",
    "effective_metric_params_ = Additional keyword arguments for the metric function.\n",
    "\n",
    "Additional attributes of the classifier:\n",
    "classes_ = Class labels known to the classifier\n",
    "outputs_2d_ = False when y’s shape is (n_samples, ) or (n_samples, 1) during fit otherwise True.\n",
    "\n",
    "Methods:\n",
    "fit(X, y) = Fit the model using X as training data and y as target values\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "kneighbors([X, n_neighbors, return_distance]) = Finds the K-neighbors of a point.\n",
    "kneighbors_graph([X, n_neighbors, mode]) = Computes the (weighted) graph of k-Neighbors for points in X\n",
    "predict(X) = Predict the class labels for the provided data.\n",
    "predict_proba(X) = CLASSIFIER ONLY!!! Return probability estimates for the test data X.\n",
    "score(X, y[, sample_weight]) = Return the mean accuracy on the given test data and labels.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "\"\"\"\n",
    "print() # Needed to supress the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ridge-reg_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "model = Ridge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, \n",
    "              tol=0.001, solver='auto', random_state=None)\n",
    "\"\"\"\n",
    "Summary:\n",
    "A supervised learning model aiming to predict a value based on the features. Made to deal with the problem of multicolinearity.\n",
    "\n",
    "Parameters:\n",
    "alpha = Regularization strength. Regularization improves the conditioning of the problem and reduces the variance of the\n",
    "    estimates. Larger values specify stronger regularization. If an array is passed, penalties are assumed to be specific \n",
    "    to the targets.\n",
    "fit_intercept = Whether to fit the intercept for this model.\n",
    "normalize = This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be \n",
    "    normalized before regression by subtracting the mean and dividing by the l2-norm. \n",
    "copy_X = If True, X will be copied; else, it may be overwritten.\n",
    "max_iter = Maximum number of iterations for conjugate gradient solver. Default value depends on the solver.\n",
    "tol = Precision of the solution.\n",
    "solver{‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’, ‘saga’} = Solver to use in the computational routines:\n",
    "    ‘auto’ chooses the solver automatically based on the type of data.\n",
    "    ‘svd’ uses a Singular Value Decomposition of X to compute the Ridge coefficients. \n",
    "        More stable for singular matrices than ‘cholesky’.\n",
    "    ‘cholesky’ uses the standard scipy.linalg.solve function to obtain a closed-form solution.\n",
    "    ‘sparse_cg’ uses the conjugate gradient solver as found in scipy.sparse.linalg.cg. As an iterative algorithm, \n",
    "        this solver is more appropriate than ‘cholesky’ for large-scale data (possibility to set tol and max_iter).\n",
    "    ‘lsqr’ uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr. It is the fastest and \n",
    "        uses an iterative procedure.\n",
    "    ‘sag’ uses a Stochastic Average Gradient descent, and ‘saga’ uses its improved, unbiased version named SAGA. \n",
    "        Both methods also use an iterative procedure, and are often faster than other solvers when both n_samples and \n",
    "        n_features are large. Note that ‘sag’ and ‘saga’ fast convergence is only guaranteed on features with approximately\n",
    "        the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.\n",
    "    All last five solvers support both dense and sparse data. However, only ‘sag’ and ‘sparse_cg’ supports sparse input \n",
    "    when fit_intercept is True.\n",
    "random_state = (When solver=‘sag’ or ‘saga’) Shuffle the data. \n",
    "\n",
    "Attributes:\n",
    "coef_ = Weight vector(s).\n",
    "intercept_ = Independent term in decision function.\n",
    "n_iter_ = Actual number of iterations for each target. Available only for sag and lsqr solvers.\n",
    "\n",
    "Methods:\n",
    "fit(X, y[, sample_weight]) = Fit linear model.\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "predict(X) = Predict using the linear model.\n",
    "score(X, y[, sample_weight]) = Return the coefficient of determination R^2 of the prediction.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lasso_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "model = Lasso(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, \n",
    "              tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
    "\"\"\"\n",
    "Summary:\n",
    "Supervised linear regression that selects features by shrinking co-efficient towards zero and avoids over fitting, but \n",
    "selected features will be highly biased\n",
    "\n",
    "Parameters:\n",
    "alpha = Regularization strength. Regularization improves the conditioning of the problem and reduces the variance of the\n",
    "    estimates. Larger values specify stronger regularization. If an array is passed, penalties are assumed to be specific \n",
    "    to the targets.\n",
    "fit_intercept = Whether to fit the intercept for this model.\n",
    "normalize = This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be \n",
    "    normalized before regression by subtracting the mean and dividing by the l2-norm. \n",
    "precompute = Whether to use a precomputed Gram matrix to speed up calculations.  The Gram matrix can also be passed as argument.\n",
    "    For sparse input this option is always True to preserve sparsity.\n",
    "copy_X = If True, X will be copied; else, it may be overwritten.\n",
    "max_iter = Maximum number of iterations for conjugate gradient solver. Default value depends on the solver.\n",
    "tol = Precision of the solution.\n",
    "warm_start = When set to True, reuse the solution of the previous call to fit as initialization, or erase the previous solution.\n",
    "positive = When set to True, forces the coefficients to be positive.\n",
    "random_state = Shuffle the data. \n",
    "selection{‘cyclic’, ‘random’} = If set to ‘random’, a random coefficient is updated every iteration rather than \n",
    "    looping over features sequentially by default. This (setting to ‘random’) often leads to significantly faster\n",
    "    convergence especially when tol is higher than 1e-4.\n",
    "\n",
    "Attributes:\n",
    "coef_ = Weight vector(s)\n",
    "sparse_coef_ = Sparse representation of the fitted coef_\n",
    "intercept_ = Independent term in decision function.\n",
    "n_iter_ = Actual number of iterations for each target. Available only for sag and lsqr solvers.\n",
    "\n",
    "Methods:\n",
    "fit(X, y[, sample_weight]) = Fit linear model.\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "path(X, y, *[, l1_ratio, eps, n_alphas, …]) = Compute elastic net path with coordinate descent.\n",
    "predict(X) = Predict using the linear model.\n",
    "score(X, y[, sample_weight]) = Return the coefficient of determination R^2 of the prediction.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='elastic_net_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "model = ElasticNet(alpha=1.0, l1_ratio=0.5, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, copy_X=True, \n",
    "                   tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
    "\"\"\"\n",
    "Summary:\n",
    "Enhancement of LASSO that doesn’t have the problem of selecting more than n predictors when n<<p, but is more computionally \n",
    "expensive.\n",
    "\n",
    "Parameters:\n",
    "alpha = Regularization strength. Regularization improves the conditioning of the problem and reduces the variance of the\n",
    "    estimates. Larger values specify stronger regularization. If an array is passed, penalties are assumed to be specific \n",
    "    to the targets.\n",
    "l1_ratio = The ElasticNet mixing parameter, with 0 <= l1_ratio <= 1. For l1_ratio = 0 the penalty is an L2 penalty. \n",
    "    For l1_ratio = 1 it is an L1 penalty. For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n",
    "fit_intercept = Whether to fit the intercept for this model.\n",
    "normalize = This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be \n",
    "    normalized before regression by subtracting the mean and dividing by the l2-norm. \n",
    "precompute = Whether to use a precomputed Gram matrix to speed up calculations.  The Gram matrix can also be passed as argument.\n",
    "    For sparse input this option is always True to preserve sparsity.\n",
    "copy_X = If True, X will be copied; else, it may be overwritten.\n",
    "max_iter = Maximum number of iterations for conjugate gradient solver. Default value depends on the solver.\n",
    "tol = Precision of the solution.\n",
    "warm_start = When set to True, reuse the solution of the previous call to fit as initialization, or erase the previous solution.\n",
    "positive = When set to True, forces the coefficients to be positive.\n",
    "random_state = Shuffle the data. \n",
    "selection{‘cyclic’, ‘random’} = If set to ‘random’, a random coefficient is updated every iteration rather than \n",
    "    looping over features sequentially by default. This (setting to ‘random’) often leads to significantly faster\n",
    "    convergence especially when tol is higher than 1e-4.\n",
    "\n",
    "Attributes:\n",
    "coef_ = Weight vector(s)\n",
    "sparse_coef_ = Sparse representation of the fitted coef_\n",
    "intercept_ = Independent term in decision function.\n",
    "n_iter_ = Actual number of iterations for each target. Available only for sag and lsqr solvers.\n",
    "\n",
    "Methods:\n",
    "fit(X, y[, sample_weight]) = Fit linear model.\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "path(X, y, *[, l1_ratio, eps, n_alphas, …]) = Compute elastic net path with coordinate descent.\n",
    "predict(X) = Predict using the linear model.\n",
    "score(X, y[, sample_weight]) = Return the coefficient of determination R^2 of the prediction.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lars_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lars\n",
    "model = Lars(fit_intercept=True, verbose=False, normalize=True, precompute='auto', \n",
    "             n_nonzero_coefs=500, eps=2.220446049250313e-16, copy_X=True, fit_path=True, jitter=None, random_state=None)\n",
    "\"\"\"\n",
    "Summary:\n",
    "It is a linear regression model selection method.\n",
    "\n",
    "Parameters:\n",
    "fit_intercept = Whether to fit the intercept for this model.\n",
    "verbose = Sets the verbosity amount\n",
    "normalize = This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be \n",
    "    normalized before regression by subtracting the mean and dividing by the l2-norm. \n",
    "precompute = Whether to use a precomputed Gram matrix to speed up calculations.  The Gram matrix can also be passed as argument.\n",
    "    For sparse input this option is always True to preserve sparsity.\n",
    "n_nonzero_coefs = Target number of non-zero coefficients. Use np.inf for no limit.\n",
    "eps = The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very \n",
    "    ill-conditioned systems. Unlike the tol parameter in some iterative optimization-based algorithms, \n",
    "    this parameter does not control the tolerance of the optimization. By default, np.finfo(np.float).eps is used.\n",
    "copy_X = If True, X will be copied; else, it may be overwritten.\n",
    "max_iter = Maximum number of iterations for conjugate gradient solver. Default value depends on the solver.\n",
    "fit_path = If True the full path is stored in the coef_path_ attribute. If you compute the solution for a \n",
    "    large problem or many targets, setting fit_path to False will lead to a speedup, especially with a small alpha.\n",
    "jitter = Upper bound on a uniform noise parameter to be added to the y values, to satisfy the model’s assumption \n",
    "    of one-at-a-time computations. Might help with stability.\n",
    "random_state = Shuffle the data. \n",
    "\n",
    "Attributes:\n",
    "alphas_ = Maximum of covariances (in absolute value) at each iteration. n_alphas is either n_nonzero_coefs or \n",
    "    n_features, whichever is smaller.\n",
    "active_ = Indices of active variables at the end of the path.\n",
    "coef_path_ = The varying values of the coefficients along the path. It is not present if the fit_path parameter is False.\n",
    "coef_ = Parameter vector (w in the formulation formula).\n",
    "intercept_ = Independent term in decision function.\n",
    "n_iter_ = The number of iterations taken by lars_path to find the grid of alphas for each target.\n",
    "\n",
    "Methods:\n",
    "fit(X, y[, sample_weight]) = Fit linear model.\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "predict(X) = Predict using the linear model.\n",
    "score(X, y[, sample_weight]) = Return the coefficient of determination R^2 of the prediction.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tree_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
    "                               min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None,\n",
    "                               min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, ccp_alpha=0.0)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "model = DecisionTreeRegressor(criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
    "                              min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None,\n",
    "                              min_impurity_decrease=0.0, min_impurity_split=None, ccp_alpha=0.0)\n",
    "\n",
    "\"\"\"\n",
    "Summary:\n",
    "A supervised algorithm that learns by creating nodes, which can be split into two paths based on the data given. The algorithm\n",
    "tries to pick best 'questions' the nodes use to perform classification or regression. They are easy to understand and \n",
    "have a very simple visual interpretation, they require little preprocessing and automatically pick best features. However, they \n",
    "have a tendency to overfit.\n",
    "\n",
    "Parameters:\n",
    "criterion{“gini”, “entropy”} (CLASSIFIER) {“mse”, “friedman_mse”, “mae”} (REGRESSOR) = The function to measure the quality of \n",
    "    a split.\n",
    "splitter{“best”, “random”} = The strategy used to choose the split at each node. \n",
    "max_depth = The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all \n",
    "    leaves contain less than min_samples_split samples.\n",
    "min_samples_split = The minimum number of samples required to split an internal node:\n",
    "min_samples_leaf = The minimum number of samples required to be at a leaf node.\n",
    "min_weight_fraction_leaf = The minimum weighted fraction of the sum total of weights (of all the input samples) \n",
    "    required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
    "max_features = The number of features to consider when looking for the best split.\n",
    "random_state = Controls the randomness of the estimator. \n",
    "max_leaf_nodes = Grow a tree with max_leaf_nodes in best-first fashion. \n",
    "min_impurity_decrease =Nodes will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "min_impurity_split = Threshold for early stopping in tree growth.\n",
    "class_weight (CLASSIFIER ONLY) = Weights associated with classes in the form {class_label: weight}.\n",
    "ccp_alpha = Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost \n",
    "    complexity that is smaller than ccp_alpha will be chosen. \n",
    "    \n",
    "Attributes:\n",
    "feature_importances_ = Return the feature importances.\n",
    "max_features_ = The inferred value of max_features.\n",
    "n_features_ = The number of features when fit is performed.\n",
    "n_outputs_ = The number of outputs when fit is performed.\n",
    "tree_ = The underlying Tree object.\n",
    "Classifier only attributes:\n",
    "classes_ = The classes labels (single output problem), or a list of arrays of class labels (multi-output problem).\n",
    "n_classes_ = The number of classes (for single output problems), or a list containing the number of classes for each output\n",
    "    (for multi-output problems).\n",
    "\n",
    "Methods:\n",
    "apply(X[, check_input]) = Return the index of the leaf that each sample is predicted as.\n",
    "cost_complexity_pruning_path(X, y[, …]) = Compute the pruning path during Minimal Cost-Complexity Pruning.\n",
    "decision_path(X[, check_input]) = Return the decision path in the tree.\n",
    "fit(X, y[, sample_weight, check_input, …]) = Build a decision tree classifier from the training set (X, y).\n",
    "get_depth() = Return the depth of the decision tree.\n",
    "get_n_leaves() = Return the number of leaves of the decision tree.\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "predict(X[, check_input]) = Predict class or regression value for X.\n",
    "score(X, y[, sample_weight]) = Return the mean accuracy on the given test data and labels.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "Classifier only methods:\n",
    "predict_log_proba(X) = Predict class log-probabilities of the input samples X.\n",
    "predict_proba(X[, check_input]) = Predict class probabilities of the input samples X.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nb_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "model = CategoricalNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "\"\"\"\n",
    "Summary:\n",
    "A supervised classification algorithm that can quickly achieve good results if the samples are independent from each other. \n",
    "The features must be categorical.\n",
    "\n",
    "Parameters:\n",
    "alpha = Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n",
    "fit_prior = Whether to learn class prior probabilities or not. If false, a uniform prior will be used.\n",
    "class_prior = Prior probabilities of the classes. If specified the priors are not adjusted according to the data.\n",
    "    \n",
    "Attributes:\n",
    "category_count_ = Holds arrays of shape (n_classes, n_categories of respective feature) for each feature. \n",
    "    Each array provides the number of samples encountered for each class and category of the specific feature.\n",
    "class_count_ = Number of samples encountered for each class during fitting. This value is weighted by the sample \n",
    "    weight when provided.\n",
    "class_log_prior_ = Smoothed empirical log probability for each class.\n",
    "classes_ = Class labels known to the classifier\n",
    "feature_log_prob_ = Holds arrays of shape (n_classes, n_categories of respective feature) for each feature. Each array\n",
    "    provides the empirical log probability of categories given the respective feature and class, P(x_i|y).\n",
    "n_features_ = Number of features of each sample.\n",
    "\n",
    "Methods:\n",
    "fit(X, y[, sample_weight]) = Fit Naive Bayes classifier according to X, y\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "partial_fit(X, y[, classes, sample_weight]) = Incremental fit on a batch of samples.\n",
    "predict(X) = Perform classification on an array of test vectors X.\n",
    "predict_log_proba(X) = Return log-probability estimates for the test vector X.\n",
    "predict_proba(X) = Return probability estimates for the test vector X.\n",
    "score(X, y[, sample_weight]) = Return the mean accuracy on the given test data and labels.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gnb_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB(priors=None, var_smoothing=1e-09)\n",
    "\"\"\"\n",
    "Summary:\n",
    "A modification of Naive Bayes that doesn't require categorical features.\n",
    "\n",
    "Parameters:\n",
    "priors = Prior probabilities of the classes. If specified the priors are not adjusted according to the data.\n",
    "var_smoothing = Portion of the largest variance of all features that is added to variances for calculation stability. \n",
    "    \n",
    "Attributes:\n",
    "class_count_ = Number of training samples observed in each class.\n",
    "class_prior_ = Probability of each class.\n",
    "classes_ = Class labels known to the classifier\n",
    "epsilon_ = Absolute additive value to variances\n",
    "sigma_ = Variance of each feature per class\n",
    "theta_ = Mean of each feature per class\n",
    "\n",
    "Methods:\n",
    "fit(X, y[, sample_weight]) = Fit Gaussian Naive Bayes according to X, y\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "partial_fit(X, y[, classes, sample_weight]) = Incremental fit on a batch of samples.\n",
    "predict(X) = Perform classification on an array of test vectors X.\n",
    "predict_log_proba(X) = Return log-probability estimates for the test vector X.\n",
    "predict_proba(X) = Return probability estimates for the test vector X.\n",
    "score(X, y[, sample_weight]) = Return the mean accuracy on the given test data and labels.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mnb_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "\"\"\"\n",
    "Summary:\n",
    "A modification of categorical Naive Bayes that needs Gaussian distribution of features.\n",
    "\n",
    "Parameters:\n",
    "alpha = Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n",
    "fit_prior = Whether to learn class prior probabilities or not. If false, a uniform prior will be used.\n",
    "class_prior = Prior probabilities of the classes. If specified the priors are not adjusted according to the data.\n",
    "    \n",
    "Attributes:\n",
    "class_count_ = Number of samples encountered for each class during fitting. This value is weighted by the sample weight \n",
    "    when provided.\n",
    "class_log_prior_ = Smoothed empirical log probability for each class.\n",
    "classes_ = Class labels known to the classifier\n",
    "coef_ = Mirrors feature_log_prob_ for interpreting MultinomialNB as a linear model.\n",
    "feature_count_ = Number of samples encountered for each (class, feature) during fitting. This value is weighted by the \n",
    "    sample weight when provided.\n",
    "feature_log_prob_ = Empirical log probability of features given a class, P(x_i|y).\n",
    "intercept_ = Mirrors class_log_prior_ for interpreting MultinomialNB as a linear model.\n",
    "n_features_ = Number of features of each sample.\n",
    "\n",
    "Methods:\n",
    "fit(X, y[, sample_weight]) = Fit Naive Bayes classifier according to X, y\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "partial_fit(X, y[, classes, sample_weight]) = Incremental fit on a batch of samples.\n",
    "predict(X) = Perform classification on an array of test vectors X.\n",
    "predict_log_proba(X) = Return log-probability estimates for the test vector X.\n",
    "predict_proba(X) = Return probability estimates for the test vector X.\n",
    "score(X, y[, sample_weight]) = Return the mean accuracy on the given test data and labels.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='k_means_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, \n",
    "               verbose=0, random_state=None, copy_x=True, algorithm='auto')\n",
    "\"\"\"\n",
    "Summary:\n",
    "An unsupervised learning algorithm that splits the data into categories. It is easy to implement and works well even on \n",
    "large datasets, however it has trouble when clusters have different sizes and density, it's also sensitive to outliers.\n",
    "\n",
    "Parameters:\n",
    "n_clusters = The number of clusters to form as well as the number of centroids to generate.\n",
    "init{‘k-means++’, ‘random’, ndarray, callable} = Method for initialization:\n",
    "    ‘k-means++’ : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence.\n",
    "    ‘random’: choose n_clusters observations (rows) at random from data for the initial centroids.\n",
    "    If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.\n",
    "    If a callable is passed, it should take arguments X, n_clusters and a random state and return an initialization.\n",
    "n_init = Number of time the k-means algorithm will be run with different centroid seeds. \n",
    "max_iter = Maximum number of iterations of the k-means algorithm for a single run.\n",
    "tol = Relative tolerance with regards to Frobenius norm of the difference in the cluster centers of two consecutive \n",
    "    iterations to declare convergence.\n",
    "verbose = Verbosity mode.\n",
    "random_state = Determines random number generation for centroid initialization. Use an int to make the randomness deterministic.\n",
    "copy_x = When pre-computing distances it is more numerically accurate to center the data first.\n",
    "    If copy_x is True (default), then the original data is not modified. If False, the original data is modified,\n",
    "    and put back before the function returns, but small numerical differences may be introduced by subtracting and then \n",
    "    adding the data mean. Note that if the original data is not C-contiguous, a copy will be made even if copy_x is False. \n",
    "algorithm{“auto”, “full”, “elkan”} = K-means algorithm to use. The classical EM-style algorithm is “full”. \n",
    "    The “elkan” variation is more efficient on data with well-defined clusters, by using the triangle inequality. \n",
    "    However it’s more memory intensive due to the allocation of an extra array of shape (n_samples, n_clusters).\n",
    "\n",
    "Attributes:\n",
    "cluster_centers_ = Coordinates of cluster centers. If the algorithm stops before fully converging (see tol and max_iter),\n",
    "    these will not be consistent with labels_.\n",
    "labels_ = Labels of each point\n",
    "inertia_ = Sum of squared distances of samples to their closest cluster center.\n",
    "n_iter_ = Number of iterations run.\n",
    "\n",
    "Methods:\n",
    "fit(X[, y, sample_weight]) = Compute k-means clustering.\n",
    "fit_predict(X[, y, sample_weight]) = Compute cluster centers and predict cluster index for each sample.\n",
    "fit_transform(X[, y, sample_weight]) = Compute clustering and transform X to cluster-distance space.\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "predict(X[, sample_weight]) = Predict the closest cluster each sample in X belongs to.\n",
    "score(X[, y, sample_weight]) = Opposite of the value of X on the K-means objective.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "transform(X) = Transform X to a cluster-distance space.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='HC_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "model = AgglomerativeClustering(n_clusters=2, affinity='euclidean', memory=None, connectivity=None, \n",
    "                                compute_full_tree='auto', linkage='ward', distance_threshold=None)\n",
    "\"\"\"\n",
    "Summary:\n",
    "Algorithm that creates clusters by pariring existing ones into pair and moving up hierarchically.\n",
    "\n",
    "Parameters:\n",
    "n_clusters = The number of clusters to find. It must be None if distance_threshold is not None.\n",
    "affinity = Metric used to compute the linkage. Can be “euclidean”, “l1”, “l2”, “manhattan”, “cosine”, or “precomputed”.\n",
    "    If linkage is “ward”, only “euclidean” is accepted. If “precomputed”, a distance matrix (instead of a similarity matrix)\n",
    "    is needed as input for the fit method.\n",
    "memory = Used to cache the output of the computation of the tree. By default, no caching is done. If a string is given, \n",
    "    it is the path to the caching directory.\n",
    "connectivity = Connectivity matrix. Defines for each sample the neighboring samples following a given structure of the data. \n",
    "    This can be a connectivity matrix itself or a callable that transforms the data into a connectivity matrix, such as derived \n",
    "    from kneighbors_graph. Default is None, i.e, the hierarchical clustering algorithm is unstructured.\n",
    "compute_full_tree = Stop early the construction of the tree at n_clusters. This is useful to decrease computation time if \n",
    "    the number of clusters is not small compared to the number of samples. This option is useful only when specifying a \n",
    "    connectivity matrix. Note also that when varying the number of clusters and using caching, it may be advantageous to \n",
    "    compute the full tree. It must be True if distance_threshold is not None. By default compute_full_tree is “auto”, \n",
    "    which is equivalent to True when distance_threshold is not None or that n_clusters is inferior to the maximum between \n",
    "    100 or 0.02 * n_samples. Otherwise, “auto” is equivalent to False.\n",
    "linkage{“ward”, “complete”, “average”, “single”} = Which linkage criterion to use. The linkage criterion determines which\n",
    "    distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion.\n",
    "    'ward' minimizes the variance of the clusters being merged.\n",
    "    'average' uses the average of the distances of each observation of the two sets.\n",
    "    'complete' or maximum linkage uses the maximum distances between all observations of the two sets.\n",
    "    'single' uses the minimum of the distances between all observations of the two sets.\n",
    "distance_threshold = The linkage distance threshold above which, clusters will not be merged. If not None, \n",
    "    n_clusters must be None and compute_full_tree must be True.\n",
    "    \n",
    "Attributes:\n",
    "n_clusters_=The number of clusters found by the algorithm. If distance_threshold=None, it will be equal to the given n_clusters.\n",
    "labels_ = cluster labels for each point\n",
    "n_leaves_ = Number of leaves in the hierarchical tree.\n",
    "n_connected_components_ = The estimated number of connected components in the graph.\n",
    "children_ = The children of each non-leaf node. Values less than n_samples correspond to leaves of the tree which are the\n",
    "    original samples. A node i greater than or equal to n_samples is a non-leaf node and has children children_[i - n_samples].\n",
    "    Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node n_samples + i\n",
    "\n",
    "Methods:\n",
    "fit(X[, y]) = Fit the hierarchical clustering from features, or distance matrix.\n",
    "fit_predict(X[, y]) = Fit the hierarchical clustering from features or distance matrix, and return cluster labels.\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='perceptron_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "model = Perceptron(penalty=None, alpha=0.0001, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0,\n",
    "                   eta0=1.0, n_jobs=None, random_state=0, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, \n",
    "                   class_weight=None, warm_start=False)\n",
    "\"\"\"\n",
    "Summary:\n",
    "Perceptron is a single layer neural network, which means it takes inputs multiplies by weights, creates a weighted sum,\n",
    "applies bias and returns a binary output. It is easy to apply and has many application but its hyper-parameters can be \n",
    "difficult to tune.\n",
    "\n",
    "Parameters:\n",
    "penalty{‘l2’,’l1’,’elasticnet’} = The penalty (aka regularization term) to be used.\n",
    "alpha = Constant that multiplies the regularization term if regularization is used.\n",
    "fit_intercept = Whether the intercept should be estimated or not. If False, the data is assumed to be already centered.\n",
    "max_iter = The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the fit method, \n",
    "    and not the partial_fit method.\n",
    "tol = The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol).\n",
    "shuffle = Whether or not the training data should be shuffled after each epoch.\n",
    "verbose = The verbosity level\n",
    "eta0 = Constant by which the updates are multiplied.\n",
    "n_jobs = The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. None means 1 \n",
    "    unless in a joblib.parallel_backend context. -1 means using all processors. \n",
    "random_state = Used to shuffle the training data, when shuffle is set to True. Pass an int for reproducible output across \n",
    "    multiple function calls.\n",
    "early_stopping = Whether to use early stopping to terminate training when validation. score is not improving. If set to \n",
    "    True, it will automatically set aside a stratified fraction of training data as validation and terminate training \n",
    "    when validation score is not improving by at least tol for n_iter_no_change consecutive epochs.\n",
    "validation_fraction =The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1.\n",
    "n_iter_no_change = Number of iterations with no improvement to wait before early stopping.\n",
    "class_weight = Preset for the class_weight fit parameter.\n",
    "warm_start = When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just \n",
    "    erase the previous solution.\n",
    "    \n",
    "Attributes:\n",
    "coef_ = Weights assigned to the features.\n",
    "intercept_ = Constants in decision function.\n",
    "n_iter_ = The actual number of iterations to reach the stopping criterion. For multiclass fits, it is the maximum over \n",
    "    every binary fit.\n",
    "classes_ = The unique classes labels.\n",
    "t_ = Number of weight updates performed during training.\n",
    "\n",
    "Methods:\n",
    "decision_function(X) = Predict confidence scores for samples.\n",
    "densify() = Convert coefficient matrix to dense array format.\n",
    "fit(X, y[, coef_init, intercept_init, …]) = Fit linear model with Stochastic Gradient Descent.\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "partial_fit(X, y[, classes, sample_weight]) = Perform one epoch of stochastic gradient descent on given samples.\n",
    "predict(X) = Predict class labels for samples in X.\n",
    "score(X, y[, sample_weight]) = Return the mean accuracy on the given test data and labels.\n",
    "set_params(**kwargs) = Set and validate the parameters of estimator.\n",
    "sparsify() = Convert coefficient matrix to sparse format.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='back_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier(hidden_layer_sizes=(100, ), activation='relu', solver='adam', alpha=0.0001, batch_size='auto',\n",
    "                      learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True,\n",
    "                      random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n",
    "                      early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08,\n",
    "                      n_iter_no_change=10, max_fun=15000)\n",
    "\"\"\"\n",
    "Summary:\n",
    "It's a classifier that uses back propagation. Backpropagation allows better gradient calculation for multiple layer networks. It \n",
    "calculate backward from the last layer to optimize the process.\n",
    "\n",
    "Parameters:\n",
    "hidden_layer_sizes = The ith element represents the number of neurons in the ith hidden layer.\n",
    "activation{‘identity’, ‘logistic’, ‘tanh’, ‘relu’} = Activation function for the hidden layer.\n",
    "    ‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x\n",
    "    ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n",
    "    ‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).\n",
    "    ‘relu’, the rectified linear unit function, returns f(x) = max(0, x)\n",
    "    solver{‘lbfgs’, ‘sgd’, ‘adam’} = The solver for weight optimization.\n",
    "    ‘lbfgs’ is an optimizer in the family of quasi-Newton methods.\n",
    "    ‘sgd’ refers to stochastic gradient descent.\n",
    "    ‘adam’ refers to a stochastic gradient-based optimizer proposed\n",
    "alpha = L2 penalty (regularization term) parameter.\n",
    "batch_size = Size of minibatches for stochastic optimizers. If the solver is ‘lbfgs’, the classifier will not use minibatch.\n",
    "    When set to “auto”, batch_size=min(200, n_samples)\n",
    "learning_rate{‘constant’, ‘invscaling’, ‘adaptive’} = Learning rate schedule for weight updates.\n",
    "    ‘constant’ is a constant learning rate given by ‘learning_rate_init’.\n",
    "    ‘invscaling’ gradually decreases the learning rate at each time step ‘t’ using an inverse scaling exponent of ‘power_t’.\n",
    "        effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
    "    ‘adaptive’ keeps the learning rate constant to ‘learning_rate_init’ as long as training loss keeps decreasing. Each \n",
    "        time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score \n",
    "        by at least tol if ‘early_stopping’ is on, the current learning rate is divided by 5.\n",
    "learning_rate_init = The initial learning rate used. It controls the step-size in updating the weights. Only used when \n",
    "    solver=’sgd’ or ‘adam’.\n",
    "power_t = The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the \n",
    "    learning_rate is set to ‘invscaling’. Only used when solver=’sgd’.\n",
    "max_iter = Maximum number of iterations. The solver iterates until convergence (determined by ‘tol’) or this number\n",
    "    of iterations. For stochastic solvers (‘sgd’, ‘adam’), note that this determines the number of epochs \n",
    "    (how many times each data point will be used), not the number of gradient steps.\n",
    "shuffle = Whether to shuffle samples in each iteration. Only used when solver=’sgd’ or ‘adam’.\n",
    "random_state = Determines random number generation for weights and bias initialization, train-test split if early \n",
    "    stopping is used, and batch sampling when solver=’sgd’ or ‘adam’. Pass an int for reproducible results across \n",
    "    multiple function calls.\n",
    "tol = Tolerance for the optimization. When the loss or score is not improving by at least tol for n_iter_no_change \n",
    "    consecutive iterations, unless learning_rate is set to ‘adaptive’, convergence is considered to be reached and\n",
    "    training stops.\n",
    "verbose = Whether to print progress messages to stdout.\n",
    "warm_start = When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just \n",
    "    erase the previous solution. \n",
    "momentum = Momentum for gradient descent update. Should be between 0 and 1. Only used when solver=’sgd’.\n",
    "nesterovs_momentum = Whether to use Nesterov’s momentum. Only used when solver=’sgd’ and momentum > 0.\n",
    "early_stopping = Whether to use early stopping to terminate training when validation score is not improving. If \n",
    "    set to true, it will automatically set aside 10% of training data as validation and terminate training when \n",
    "    validation score is not improving by at least tol for n_iter_no_change consecutive epochs. The split is stratified, \n",
    "    except in a multilabel setting. Only effective when solver=’sgd’ or ‘adam’\n",
    "validation_fraction =The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1.\n",
    "beta_1 = Exponential decay rate for estimates of first moment vector in adam, should be in [0, 1). Only used when solver=’adam’\n",
    "beta_2 = Exponential decay rate for estimates of second moment vector in adam, should be in [0, 1). Only used when solver=’adam’\n",
    "epsilon = Value for numerical stability in adam. Only used when solver=’adam’\n",
    "n_iter_no_change = Maximum number of epochs to not meet tol improvement. Only effective when solver=’sgd’ or ‘adam’\n",
    "max_fun = Only used when solver=’lbfgs’. Maximum number of loss function calls. The solver iterates until\n",
    "    convergence (determined by ‘tol’), number of iterations reaches max_iter, or this number of loss function calls. \n",
    "    Note that number of loss function calls will be greater than or equal to the number of iterations for the MLPClassifier.\n",
    "    \n",
    "Attributes:\n",
    "classes_ = Class labels for each output.\n",
    "loss_ = The current loss computed with the loss function.\n",
    "coefs_ = The ith element in the list represents the weight matrix corresponding to layer i.\n",
    "intercepts_ = The ith element in the list represents the bias vector corresponding to layer i + 1.\n",
    "n_iter_ = The number of iterations the solver has ran.\n",
    "n_layers_ = Number of layers.\n",
    "n_outputs_ = Number of outputs.\n",
    "out_activation_ = Name of the output activation function.\n",
    "\n",
    "Methods:\n",
    "fit(X, y) = Fit the model to data matrix X and target(s) y.\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "predict(X) = Predict using the multi-layer perceptron classifier\n",
    "predict_log_proba(X) = Return the log of probability estimates.\n",
    "predict_proba(X) = Probability estimates.\n",
    "score(X, y[, sample_weight]) = Return the mean accuracy on the given test data and labels.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dbm_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import BernoulliRBM\n",
    "model = BernoulliRBM(n_components=256, learning_rate=0.1, batch_size=10, n_iter=10,\n",
    "                     verbose=0, random_state=None)\n",
    "\"\"\"\n",
    "Summary:\n",
    "Restricted Boltzmann Machine is a supervised stochastic algorithm that can learn that can learn a probability distribution over \n",
    "inputs and then generate similar samples.\n",
    "\n",
    "Parameters:\n",
    "n_components = Number of binary hidden units.\n",
    "learning_rate = The learning rate for weight updates. It is highly recommended to tune this hyper-parameter. \n",
    "    Reasonable values are in the 10**[0., -3.] range.\n",
    "batch_size = Number of examples per minibatch.\n",
    "n_iter = Number of iterations/sweeps over the training dataset to perform during training.\n",
    "verbose = The verbosity level. The default, zero, means silent mode.\n",
    "random_state = Determines random number generation for:\n",
    "    Gibbs sampling from visible and hidden layers.\n",
    "    Initializing components, sampling from layers during fit.\n",
    "    Corrupting the data when scoring samples.\n",
    "    Pass an int for reproducible results across multiple function calls\n",
    "    \n",
    "Attributes:\n",
    "intercept_hidden_ = Biases of the hidden units.\n",
    "intercept_visible_ = Biases of the visible units.\n",
    "components_ = Weight matrix, where n_features in the number of visible units and n_components is the number of hidden units.\n",
    "h_samples_ = Hidden Activation sampled from the model distribution, where batch_size in the number of examples per \n",
    "    minibatch and n_components is the number of hidden units.\n",
    "\n",
    "Methods:\n",
    "fit(X[, y]) = Fit the model to the data X.\n",
    "fit_transform(X[, y]) = Fit to data, then transform it.\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "gibbs(v) = Perform one Gibbs sampling step.\n",
    "partial_fit(X[, y]) = Fit the model to the data X which should contain a partial segment of the data.\n",
    "score_samples(X) = Compute the pseudo-likelihood of X.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "transform(X) = Compute the hidden layer activation probabilities, P(h=1|v=X).\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pca_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "model = PCA(n_components=None, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)\n",
    "\"\"\"\n",
    "Summary:\n",
    "Dimensionality reduction algorithm, that aims to reduce a dataset by creating new hyper-planes with maximized variance values.\n",
    "\n",
    "Parameters:\n",
    "n_components = Number of components to keep. if n_components is not set all components are kept: \n",
    "    n_components == min(n_samples, n_features)\n",
    "copy = If False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results,\n",
    "    use fit_transform(X) instead.\n",
    "whiten = When True (False by default) the components_ vectors are multiplied by the square root of n_samples and then \n",
    "    divided by the singular values to ensure uncorrelated outputs with unit component-wise variances.Whitening will \n",
    "    remove some information from the transformed signal (the relative variance scales of the components) but can \n",
    "    sometimes improve the predictive accuracy of the downstream estimators by making their data respect some \n",
    "    hard-wired assumptions.\n",
    "svd_solver = \n",
    "    If auto : The solver is selected by a default policy based on X.shape and n_components: if the input data is larger \n",
    "        than 500x500 and the number of components to extract is lower than 80% of the smallest dimension of the data, \n",
    "        then the more efficient ‘randomized’ method is enabled. Otherwise the exact full SVD is computed and optionally \n",
    "        truncated afterwards.\n",
    "    If full : run exact full SVD calling the standard LAPACK solver via scipy.linalg.svd and select the components by \n",
    "        postprocessing\n",
    "    If arpack : run SVD truncated to n_components calling ARPACK solver via scipy.sparse.linalg.svds. It requires strictly \n",
    "        0 < n_components < min(X.shape)\n",
    "    If randomized :run randomized SVD by the method of Halko et al.\n",
    "tol = Tolerance for singular values computed by svd_solver == ‘arpack’.\n",
    "iterated_power = Number of iterations for the power method computed by svd_solver == ‘randomized’.\n",
    "random_state = Used when svd_solver == ‘arpack’ or ‘randomized’. Pass an int for reproducible results across multiple \n",
    "    function calls.\n",
    "    \n",
    "Attributes:\n",
    "components_ = Principal axes in feature space, representing the directions of maximum variance in the data. \n",
    "    The components are sorted by explained_variance_.\n",
    "explained_variance_ = The amount of variance explained by each of the selected components.\n",
    "explained_variance_ratio_ = Percentage of variance explained by each of the selected components.\n",
    "singular_values_ = The singular values corresponding to each of the selected components. The singular values are equal to \n",
    "    the 2-norms of the n_components variables in the lower-dimensional space.\n",
    "mean_ = Per-feature empirical mean, estimated from the training set.\n",
    "n_components_ = The estimated number of components. When n_components is set to ‘mle’ or a number between 0 and 1 \n",
    "    (with svd_solver == ‘full’) this number is estimated from input data. Otherwise it equals the parameter n_components,\n",
    "    or the lesser value of n_features and n_samples if n_components is None.\n",
    "n_features_ = Number of features in the training data.\n",
    "n_samples_ = Number of samples in the training data.\n",
    "noise_variance_ = The estimated noise covariance.\n",
    "\n",
    "Methods:\n",
    "fit(X[, y]) = Fit the model with X.\n",
    "fit_transform(X[, y]) = Fit the model with X and apply the dimensionality reduction on X.\n",
    "get_covariance() = Compute data covariance with the generative model.\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "get_precision() = Compute data precision matrix with the generative model.\n",
    "inverse_transform(X) = Transform data back to its original space.\n",
    "score(X[, y]) = Return the average log-likelihood of all samples.\n",
    "score_samples(X) = Return the log-likelihood of each sample.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "transform(X) = Apply dimensionality reduction to X.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plsr_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "model = PLSRegression(n_components=2, scale=True, max_iter=500, tol=1e-06, copy=True)\n",
    "\"\"\"\n",
    "Summary:\n",
    "Dimensionality reduction algorithm that bears some relation to principal components regression, however instead of \n",
    "finding hyper-planes of maximum variance between the response and independent variables, it finds a linear regression \n",
    "model by projecting the predicted variables and the observable variables to a new space. \n",
    "\n",
    "Parameters:\n",
    "n_components = Number of components to keep.\n",
    "scale = whether to scale the data\n",
    "max_iter = the maximum number of iterations of the NIPALS inner loop (used only if algorithm=”nipals”)\n",
    "tol = Tolerance used in the iterative algorithm default 1e-06.\n",
    "copy = Whether the deflation should be done on a copy. Let the default value to True unless you don’t care about side effect\n",
    "    \n",
    "Attributes:\n",
    "x_weights_ = X block weights vectors.\n",
    "y_weights_ = Y block weights vectors.\n",
    "x_loadings_ = X block loadings vectors.\n",
    "y_loadings_ = Y block loadings vectors.\n",
    "x_scores_ = X scores.\n",
    "y_scores_ = Y scores.\n",
    "x_rotations_ = X block to latents rotations.\n",
    "y_rotations_ = Y block to latents rotations.\n",
    "coef_ = The coefficients of the linear model: Y = X coef_ + Err\n",
    "n_iter_ = Number of iterations of the NIPALS inner loop for each component.\n",
    "\n",
    "Methods:\n",
    "fit(X, Y) = Fit model to data.\n",
    "fit_transform(X[, y]) = Learn and apply the dimension reduction on the train data.\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "inverse_transform(X) = Transform data back to its original space.\n",
    "predict(X[, copy]) = Apply the dimension reduction learned on the train data.\n",
    "score(X, y[, sample_weight]) = Return the coefficient of determination R^2 of the prediction.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "transform(X[, Y, copy]) = Apply the dimension reduction learned on the train data.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ms_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import MDS\n",
    "model = MDS(n_components=2,metric=True, n_init=4, max_iter=300, verbose=0, eps=0.001, n_jobs=None, \n",
    "            random_state=None, dissimilarity='euclidean')\n",
    "\"\"\"\n",
    "Summary:\n",
    "Multidimensional scaling is a dimensionality reduction algorithm that tries to find latent variables with high correlation to \n",
    "the data.\n",
    "\n",
    "Parameters:\n",
    "n_components = Number of dimensions in which to immerse the dissimilarities.\n",
    "metric = If True, perform metric MDS; otherwise, perform nonmetric MDS.\n",
    "n_init = Number of times the SMACOF algorithm will be run with different initializations. The final results will be \n",
    "    the best output of the runs, determined by the run with the smallest final stress.\n",
    "max_iter = Maximum number of iterations of the SMACOF algorithm for a single run.\n",
    "verbose = Level of verbosity.\n",
    "eps = Relative tolerance with respect to stress at which to declare convergence.\n",
    "n_jobs = The number of jobs to use for the computation. If multiple initializations are used (n_init), each run of the \n",
    "    algorithm is computed in parallel.\n",
    "random_state = Determines the random number generator used to initialize the centers. Pass an int for reproducible \n",
    "    results across multiple function calls.\n",
    "dissimilarity{‘euclidean’, ‘precomputed’} = Dissimilarity measure to use:\n",
    "    ‘euclidean’: Pairwise Euclidean distances between points in the dataset.\n",
    "    ‘precomputed’: Pre-computed dissimilarities are passed directly to fit and fit_transform.\n",
    "    \n",
    "Attributes:\n",
    "embedding_ = Stores the position of the dataset in the embedding space.\n",
    "stress_=The final value of the stress (sum of squared distance of the disparities and the distances for all constrained points).\n",
    "\n",
    "Methods:\n",
    "fit(X[, y, init]) = Computes the position of the points in the embedding space\n",
    "fit_transform(X[, y, init]) = Fit the data from X, and returns the embedded coordinates\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lda_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "model = LinearDiscriminantAnalysis(solver='svd', shrinkage=None, priors=None, n_components=None, store_covariance=False, \n",
    "                                   tol=0.0001)\n",
    "\"\"\"\n",
    "Summary:\n",
    "An algorithm that attempts to find a linear combination of features that would allow to classify them. It can be used for \n",
    "classification and dimensionality reduction.\n",
    "\n",
    "Parameters:\n",
    "solver{‘svd’, ‘lsqr’, ‘eigen’} = Solver to use, possible values:\n",
    "    ‘svd’: Singular value decomposition (default). Does not compute the covariance matrix, therefore this solver is \n",
    "        recommended for data with a large number of features.\n",
    "    ‘lsqr’: Least squares solution, can be combined with shrinkage.\n",
    "    ‘eigen’: Eigenvalue decomposition, can be combined with shrinkage.\n",
    "shrinkage = Shrinkage parameter, possible values:\n",
    "    None: no shrinkage (default).\n",
    "    ‘auto’: automatic shrinkage using the Ledoit-Wolf lemma.\n",
    "    float between 0 and 1: fixed shrinkage parameter.\n",
    "    Note that shrinkage works only with ‘lsqr’ and ‘eigen’ solvers.\n",
    "priors = The class prior probabilities. By default, the class proportions are inferred from the training data.\n",
    "n_components = Number of components (<= min(n_classes - 1, n_features)) for dimensionality reduction. If None, will\n",
    "    be set to min(n_classes - 1, n_features). This parameter only affects the transform method.\n",
    "store_covariance = If True, explicitely compute the weighted within-class covariance matrix when solver is ‘svd’. \n",
    "    The matrix is always computed and stored for the other solvers.\n",
    "tol = Absolute threshold for a singular value of X to be considered significant, used to estimate the rank of X. \n",
    "    Dimensions whose singular values are non-significant are discarded. Only used if solver is ‘svd’.\n",
    "    \n",
    "Attributes:\n",
    "coef_ = Weight vector(s).\n",
    "intercept_ = Intercept term.\n",
    "covariance_ = Weighted within-class covariance matrix. It corresponds to sum_k prior_k * C_k where C_k is the covariance\n",
    "    matrix of the samples in class k. The C_k are estimated using the (potentially shrunk) biased estimator of covariance.\n",
    "    If solver is ‘svd’, only exists when store_covariance is True.\n",
    "explained_variance_ratio_ = Percentage of variance explained by each of the selected components. If n_components is not\n",
    "    set then all components are stored and the sum of explained variances is equal to 1.0. Only available when eigen or \n",
    "    svd solver is used.\n",
    "means_ = Class-wise means.\n",
    "priors_ = Class priors (sum to 1).\n",
    "scalings_ = Scaling of the features in the space spanned by the class centroids. Only available for ‘svd’ and ‘eigen’ solvers.\n",
    "xbar_ = Overall mean. Only present if solver is ‘svd’.\n",
    "classes_ = Unique class labels.\n",
    "\n",
    "Methods:\n",
    "decision_function(X) = Apply decision function to an array of samples.\n",
    "fit(X, y) = Fit LinearDiscriminantAnalysis model according to the given\n",
    "fit_transform(X[, y]) = Fit to data, then transform it.\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "predict(X) =  = Predict class labels for samples in X.\n",
    "predict_log_proba(X) = Estimate log probability.\n",
    "predict_proba(X) = Estimate probability.\n",
    "score(X, y[, sample_weight]) = Return the mean accuracy on the given test data and labels.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "transform(X) = Project data to maximize class separation.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='qda_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "model = QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=False, tol=0.0001)\n",
    "\"\"\"\n",
    "Summary:\n",
    "It's a modification if the LDA algorithm and it allows non-linear separation of data.\n",
    "\n",
    "Parameters:\n",
    "priors = Class priors. By default, the class proportions are inferred from the training data.\n",
    "reg_param = Regularizes the per-class covariance estimates by transforming S2 as S2 = (1 - reg_param) * S2 + reg_param * \n",
    "    np.eye(n_features), where S2 corresponds to the scaling_ attribute of a given class.\n",
    "store_covariance = If True, the class covariance matrices are explicitely computed and stored in the self.covariance_ attribute.\n",
    "tol = Absolute threshold for a singular value to be considered significant, used to estimate the rank of Xk where Xk is \n",
    "    the centered matrix of samples in class k. This parameter does not affect the predictions. It only controls a \n",
    "    warning that is raised when features are considered to be colinear.\n",
    "    \n",
    "Attributes:\n",
    "covariance_ = For each class, gives the covariance matrix estimated using the samples of that class. The estimations\n",
    "    are unbiased. Only present if store_covariance is True.\n",
    "means_ = Class-wise means.\n",
    "priors_ = Class priors (sum to 1).\n",
    "rotations_ = For each class k an array of shape (n_features, n_k), where n_k = min(n_features, number of elements in class k)\n",
    "    It is the rotation of the Gaussian distribution, i.e. its principal axis. It corresponds to V, the matrix of eigenvectors\n",
    "    coming from the SVD of Xk = U S Vt where Xk is the centered matrix of samples from class k.\n",
    "scalings_ = For each class, contains the scaling of the Gaussian distributions along its principal axes, i.e. the variance \n",
    "    in the rotated coordinate system. It corresponds to S^2 / (n_samples - 1), where S is the diagonal matrix of singular \n",
    "    values from the SVD of Xk, where Xk is the centered matrix of samples from class k.\n",
    "classes_ = Unique class labels.\n",
    "\n",
    "Methods:\n",
    "decision_function(X) = Apply decision function to an array of samples.\n",
    "fit(X, y) = Fit the model according to the given training data and parameters.\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "predict(X) = Perform classification on an array of test vectors X.\n",
    "predict_log_proba(X) = Return log of posterior probabilities of classification.\n",
    "predict_proba(X) = Return posterior probabilities of classification.\n",
    "score(X, y[, sample_weight]) = Return the mean accuracy on the given test data and labels.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='BA_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "model = BaggingClassifier(base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True, \n",
    "                          bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None,verbose=0)\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "model = BaggingRegressor(base_estimator=None, n_estimators=10,max_samples=1.0, max_features=1.0, bootstrap=True,\n",
    "                         bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n",
    "\"\"\"\n",
    "Summary:\n",
    "An ensemble algorithm that trains several models on random subsets of the original datasets and then aggregates them to \n",
    "do the final prediction.\n",
    "\n",
    "Parameters:\n",
    "base_estimator =The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a decision tree.\n",
    "n_estimators = The number of base estimators in the ensemble.\n",
    "max_samples = The number of samples to draw from X to train each base estimator\n",
    "    If int, then draw max_samples samples.\n",
    "    If float, then draw max_samples * X.shape[0] samples.\n",
    "max_features = The number of features to draw from X to train each base estimator\n",
    "    If int, then draw max_features features.\n",
    "    If float, then draw max_features * X.shape[1] features.\n",
    "bootstrap = Whether samples are drawn with replacement. If False, sampling without replacement is performed.\n",
    "bootstrap_features = Whether features are drawn with replacement.\n",
    "oob_score = Whether to use out-of-bag samples to estimate the generalization error.\n",
    "warm_start = When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, \n",
    "    otherwise, just fit a whole new ensemble.\n",
    "n_jobs = The number of jobs to run in parallel for both fit and predict. None means 1 unless in a joblib.parallel_backend \n",
    "    context. -1 means using all processors.\n",
    "random_state = Controls the random resampling of the original dataset (sample wise and feature wise). If the base\n",
    "    estimator accepts a random_state attribute, a different seed is generated for each instance in the ensemble.\n",
    "    Pass an int for reproducible output across multiple function calls.\n",
    "verbose = Controls the verbosity when fitting and predicting.\n",
    "    \n",
    "Attributes:\n",
    "base_estimator_ = The base estimator from which the ensemble is grown.\n",
    "n_features_ = The number of features when fit is performed.\n",
    "estimators_ = The collection of fitted base estimators.\n",
    "estimators_samples_ = The subset of drawn samples for each base estimator.\n",
    "estimators_features_ = The subset of drawn features for each base estimator.\n",
    "classes_ (CLASSIFIER ONLY) = The classes labels.\n",
    "n_classes_ (CLASSIFIER ONLY) = The number of classes.\n",
    "oob_score_ = Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when oob_score\n",
    "    is True.\n",
    "oob_decision_function_ = Decision function computed with out-of-bag estimate on the training set. If n_estimators is small\n",
    "    it might be possible that a data point was never left out during the bootstrap. In this case, oob_decision_function_ \n",
    "    might contain NaN. This attribute exists only when oob_score is True.\n",
    "\n",
    "Methods:\n",
    "decision_function(X) (CLASSIFIER ONLY) = Average of the decision functions of the base classifiers.\n",
    "fit(X, y[, sample_weight]) = Build a Bagging ensemble of estimators from the training\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "predict(X) = Predict class for X.\n",
    "predict_log_proba(X) (CLASSIFIER ONLY) = Predict class log-probabilities for X.\n",
    "predict_proba(X) (CLASSIFIER ONLY) = Predict class probabilities for X.\n",
    "score(X, y[, sample_weight]) = Return the mean accuracy on the given test data and labels.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='SG_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('lr', LinearRegression(fit_intercept=False)),\n",
    "    ('lasso', Lasso(alpha=1.0, fit_intercept=True))] # Example of models to stack\n",
    "    \n",
    "from sklearn.ensemble import StackingClassifier\n",
    "model = StackingClassifier(estimators, final_estimator=None, cv=None, stack_method='auto', n_jobs=None, passthrough=False, \n",
    "                           verbose=0)\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "model = StackingRegressor(estimators, final_estimator=None, cv=None, n_jobs=None, passthrough=False, verbose=0)\n",
    "\"\"\"\n",
    "Summary:\n",
    "Algorithm that stacks various other models and uses the entire stack to do the final prediction.\n",
    "\n",
    "Parameters:\n",
    "estimators = Base estimators which will be stacked together. Each element of the list is defined as a tuple of string\n",
    "    (i.e. name) and an estimator instance. An estimator can be set to ‘drop’ using set_params.\n",
    "final_estimator =A classifier which will be used to combine the base estimators. The default classifier is a LogisticRegression.\n",
    "cv = Determines the cross-validation splitting strategy used in cross_val_predict to train final_estimator. Possible \n",
    "    inputs for cv are:\n",
    "    None, to use the default 5-fold cross validation,\n",
    "    integer, to specify the number of folds in a (Stratified) KFold,\n",
    "    An object to be used as a cross-validation generator,\n",
    "    An iterable yielding train, test splits.\n",
    "stack_method{‘auto’, ‘predict_proba’, ‘decision_function’, ‘predict’} (CLASSIFIER ONLY) = Methods called for each base estimator. It can be:\n",
    "    if ‘auto’, it will try to invoke, for each estimator, 'predict_proba', 'decision_function' or 'predict' in that order.\n",
    "    otherwise, one of 'predict_proba', 'decision_function' or 'predict'. If the method is not implemented by the estimator, \n",
    "        it will raise an error.\n",
    "n_jobs = The number of jobs to run in parallel all estimators fit. None means 1 unless in a joblib.parallel_backend context.\n",
    "    -1 means using all processors.\n",
    "passthrough = When False, only the predictions of estimators will be used as training data for final_estimator. \n",
    "    When True, the final_estimator is trained on the predictions as well as the original training data.\n",
    "verbose = Verbosity level.\n",
    "    \n",
    "Attributes:\n",
    "classes_ (CLASSIFIER ONLY) = Class labels.\n",
    "estimators_ = The elements of the estimators parameter, having been fitted on the training data. If an estimator has been \n",
    "    set to 'drop', it will not appear in estimators_.\n",
    "named_estimators_ = Attribute to access any fitted sub-estimators by name.\n",
    "final_estimator_ = The classifier which predicts given the output of estimators_.\n",
    "stack_method_ (CLASSIFIER ONLY) = The method used by each base estimator.\n",
    "\n",
    "Methods:\n",
    "decision_function(X) (CLASSIFIER ONLY) = Predict decision function for samples in X using final_estimator_.decision_function.\n",
    "fit(X, y[, sample_weight]) = Fit the estimators.\n",
    "fit_transform(X[, y]) = Fit to data, then transform it.\n",
    "get_params([deep]) = Get the parameters of an estimator from the ensemble.\n",
    "predict(X, **predict_params) = Predict target for X.\n",
    "predict_proba(X) (CLASSIFIER ONLY) = Predict class probabilities for X using final_estimator_.predict_proba.\n",
    "score(X, y[, sample_weight]) = Return the mean accuracy on the given test data and labels.\n",
    "set_params(**params) = Set the parameters of an estimator from the ensemble.\n",
    "transform(X) = Return class labels or probabilities for X for each estimator.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Ada_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model = AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "model = AdaBoostRegressor(base_estimator=None, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)\n",
    "\"\"\"\n",
    "Summary:\n",
    "Algorithm that creates multiple weak models to combine them into a single strong model.\n",
    "\n",
    "Parameters:\n",
    "base_estimator = The base estimator from which the boosted ensemble is built. Support for sample weighting is required, \n",
    "    as well as proper classes_ and n_classes_ attributes. If None, then the base estimator is \n",
    "    DecisionTreeClassifier(max_depth=1).\n",
    "n_estimators = The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning\n",
    "    procedure is stopped early.\n",
    "learning_rate = Learning rate shrinks the contribution of each classifier by learning_rate. There is a trade-off between \n",
    "    learning_rate and n_estimators.\n",
    "algorithm{‘SAMME’, ‘SAMME.R’} (CLASSIFIER ONLY) = If ‘SAMME.R’ then use the SAMME.R real boosting algorithm. base_estimator must support \n",
    "    calculation of class probabilities. If ‘SAMME’ then use the SAMME discrete boosting algorithm. The SAMME.R algorithm \n",
    "    typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations.\n",
    "loss{‘linear’, ‘square’, ‘exponential’} (REGRESSOR ONLY) = The loss function to use when updating the weights after each boosting iteration.\n",
    "random_state = Controls the random seed given at each base_estimator at each boosting iteration. Thus, it is only used \n",
    "    when base_estimator exposes a random_state. Pass an int for reproducible output across multiple function calls.\n",
    "    \n",
    "Attributes:\n",
    "base_estimator_ = The base estimator from which the ensemble is grown.\n",
    "estimators_ = The collection of fitted sub-estimators.\n",
    "classes_ (CLASSIFIER ONLY) = The classes labels.\n",
    "n_classes_ (CLASSIFIER ONLY) = The number of classes.\n",
    "estimator_weights_ = Weights for each estimator in the boosted ensemble.\n",
    "estimator_errors_ = Classification error for each estimator in the boosted ensemble.\n",
    "feature_importances_ = The impurity-based feature importances.\n",
    "\n",
    "Methods:\n",
    "decision_function(X) (CLASSIFIER ONLY) = Compute the decision function of X.\n",
    "fit(X, y[, sample_weight]) = Build a boosted classifier from the training set (X, y).\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "predict(X) = Predict classes for X.\n",
    "predict_log_proba(X) (CLASSIFIER ONLY) = Predict class log-probabilities for X.\n",
    "predict_proba(X) (CLASSIFIER ONLY) = Predict class probabilities for X.\n",
    "score(X, y[, sample_weight]) = Return the mean accuracy on the given test data and labels.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "staged_decision_function(X) (CLASSIFIER ONLY) = Compute decision function of X for each boosting iteration.\n",
    "staged_predict(X) = Return staged predictions for X.\n",
    "staged_predict_proba(X) (CLASSIFIER ONLY) = Predict class probabilities for X.\n",
    "staged_score(X, y[, sample_weight]) = Return staged scores for X, y.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='GBM_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, \n",
    "                                   criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, \n",
    "                                   min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0,\n",
    "                                   init=None, random_state=None, max_features=None, verbose=0, \n",
    "                                   max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, \n",
    "                                   n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor(loss='ls', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', \n",
    "                                  min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, \n",
    "                                  min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, \n",
    "                                  alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, \n",
    "                                  validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "\"\"\"\n",
    "Summary:\n",
    "It's a modification of AdaBoost lagorithm and uses weighted minimization, after which the classifiers and weighted inputs\n",
    "are recalculated. \n",
    "\n",
    "Parameters:\n",
    "loss{‘deviance’, ‘exponential’} (CLASSIFIER) {‘ls’, ‘lad’, ‘huber’, ‘quantile’} (REGRESSOR)= loss function to be optimized.\n",
    "    ‘deviance’ refers to deviance (= logistic regression) for classification with probabilistic outputs. For loss \n",
    "    ‘exponential’ gradient boosting recovers the AdaBoost algorithm. ‘ls’ refers to least squares regression. ‘lad’ \n",
    "    (least absolute deviation) is a highly robust loss function solely based on order information of the input variables. \n",
    "    ‘huber’ is a combination of the two. ‘quantile’ allows quantile regression (use alpha to specify the quantile).\n",
    "learning_rate = learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between \n",
    "    learning_rate and n_estimators.\n",
    "n_estimators = The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a \n",
    "    large number usually results in better performance.\n",
    "subsample = The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this \n",
    "    results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators. Choosing subsample \n",
    "    < 1.0 leads to a reduction of variance and an increase in bias.\n",
    "criterion{‘friedman_mse’, ‘mse’, ‘mae’} = The function to measure the quality of a split. Supported criteria are \n",
    "    ‘friedman_mse’ for the mean squared error with improvement score by Friedman, ‘mse’ for mean squared error,\n",
    "    and ‘mae’ for the mean absolute error. The default value of ‘friedman_mse’ is generally the best as it can provide\n",
    "    a better approximation in some cases.\n",
    "min_samples_split = The minimum number of samples required to split an internal node:\n",
    "    If int, then consider min_samples_split as the minimum number.\n",
    "    If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of\n",
    "        samples for each split.\n",
    "min_samples_leaf = The minimum number of samples required to be at a leaf node. A split point at any depth will only be \n",
    "    considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. \n",
    "    This may have the effect of smoothing the model, especially in regression.\n",
    "    If int, then consider min_samples_leaf as the minimum number.\n",
    "    If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples \n",
    "        for each node.\n",
    "min_weight_fraction_leaf = The minimum weighted fraction of the sum total of weights (of all the input samples) required \n",
    "    to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
    "max_depthint = maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the \n",
    "    tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables.\n",
    "min_impurity_decrease = A node will be split if this split induces a decrease of the impurity greater than or equal \n",
    "    to this value.\n",
    "init = An estimator object that is used to compute the initial predictions. init has to provide fit and predict_proba.\n",
    "    If ‘zero’, the initial raw predictions are set to zero. By default, a DummyEstimator predicting the classes priors is used.\n",
    "random_state = Controls the random seed given to each Tree estimator at each boosting iteration. In addition, it controls \n",
    "    the random permutation of the features at each split (see Notes for more details). It also controls the random spliting \n",
    "    of the training data to obtain a validation set if n_iter_no_change is not None. Pass an int for reproducible output \n",
    "    across multiple function calls.\n",
    "max_features{‘auto’, ‘sqrt’, ‘log2’} = The number of features to consider when looking for the best split:\n",
    "    If int, then consider max_features features at each split.\n",
    "    If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split.\n",
    "    If ‘auto’, then max_features=sqrt(n_features).\n",
    "    If ‘sqrt’, then max_features=sqrt(n_features).\n",
    "    If ‘log2’, then max_features=log2(n_features).\n",
    "    If None, then max_features=n_features.\n",
    "    Choosing max_features < n_features leads to a reduction of variance and an increase in bias.\n",
    "alpha (REGRESSOR ONLY) = The alpha-quantile of the huber loss function and the quantile loss function. \n",
    "    Only if loss='huber' or loss='quantile'.\n",
    "verbose = Enable verbose output. If 1 then it prints progress and performance once in a while (the more trees the lower\n",
    "    the frequency). If greater than 1 then it prints progress and performance for every tree.\n",
    "max_leaf_nodes = Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity.\n",
    "    If None then unlimited number of leaf nodes.\n",
    "warm_start = When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble,\n",
    "    otherwise, just erase the previous solution.\n",
    "validation_fraction =The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1.\n",
    "n_iter_no_change = used to decide if early stopping will be used to terminate training when validation score is not improving. \n",
    "    By default it is set to None to disable early stopping. If set to a number, it will set aside validation_fraction size of\n",
    "    the training data as validation and terminate training when validation score is not improving in all of the previous \n",
    "    n_iter_no_change numbers of iterations. The split is stratified.\n",
    "tol = Tolerance for the early stopping. When the loss is not improving by at least tol for n_iter_no_change iterations \n",
    "    (if set to a number), the training stops.\n",
    "ccp_alpha = Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity\n",
    "    that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. See Minimal Cost-Complexity Pruning\n",
    "    for details.\n",
    "    \n",
    "Attributes:\n",
    "n_estimators_ (CLASSIFIER ONLY) = The number of estimators as selected by early stopping (if n_iter_no_change is specified). Otherwise \n",
    "    it is set to n_estimators.\n",
    "feature_importances_ = The impurity-based feature importances.\n",
    "oob_improvement_ = The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration.\n",
    "    oob_improvement_[0] is the improvement in loss of the first stage over the init estimator. Only available if subsample < 1.0\n",
    "train_score_ = The i-th score train_score_[i] is the deviance (= loss) of the model at iteration i on the in-bag sample. \n",
    "    If subsample == 1 this is the deviance on the training data.\n",
    "loss_ = The concrete LossFunction object.\n",
    "init_ = The estimator that provides the initial predictions. Set via the init argument or loss.init_estimator.\n",
    "estimators_ = The collection of fitted sub-estimators. loss_.K is 1 for binary classification, otherwise n_classes.\n",
    "classes_ (CLASSIFIER ONLY) = The classes labels.\n",
    "n_features_ = The number of data features.\n",
    "n_classes_ (CLASSIFIER ONLY) = The number of classes.\n",
    "max_features_ = The inferred value of max_features.\n",
    "\n",
    "Methods:\n",
    "apply(X) = Apply trees in the ensemble to X, return leaf indices.\n",
    "decision_function(X) (CLASSIFIER ONLY) = Compute the decision function of X.\n",
    "fit(X, y[, sample_weight, monitor]) = Fit the gradient boosting model.\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "predict(X) = Predict class for X.\n",
    "predict_log_proba(X) (CLASSIFIER ONLY) = Predict class log-probabilities for X.\n",
    "predict_proba(X) (CLASSIFIER ONLY) = Predict class probabilities for X.\n",
    "score(X, y[, sample_weight]) = Return the mean accuracy on the given test data and labels.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "staged_decision_function(X) (CLASSIFIER ONLY) = Compute decision function of X for each iteration.\n",
    "staged_predict(X) = Predict class at each stage for X.\n",
    "staged_predict_proba(X) (CLASSIFIER ONLY) = Predict class probabilities at each stage for X.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='RF_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, \n",
    "                               min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, \n",
    "                               min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None,\n",
    "                               random_state=None, verbose=0, warm_start=False, class_weight=None, \n",
    "                               ccp_alpha=0.0, max_samples=None)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=100, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
    "                              min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, \n",
    "                              min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None,\n",
    "                              random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)\n",
    "\"\"\"\n",
    "Summary:\n",
    "A supervised learning algorithm, that uses bagging to create multiple decision trees and then merge them into one.\n",
    "\n",
    "Parameters:\n",
    "n_estimators = The number of trees in the forest.\n",
    "criterion{“gini”, “entropy”} (CLASSIFIER) {“mse”, “mae”} (REGRESSOR) = The function to measure the quality of a split. Supported criteria are “gini” for \n",
    "    the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific. Supported criteria are \n",
    "    “mse” for the mean squared error, which is equal to variance reduction as feature selection criterion, and “mae” \n",
    "    for the mean absolute error.\n",
    "max_depth = The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all \n",
    "    leaves contain less than min_samples_split samples.\n",
    "min_samples_split = The minimum number of samples required to split an internal node:\n",
    "    If int, then consider min_samples_split as the minimum number.\n",
    "    If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples \n",
    "        for each split.\n",
    "min_samples_leaf = The minimum number of samples required to be at a leaf node. A split point at any depth will only be \n",
    "    considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. \n",
    "    This may have the effect of smoothing the model, especially in regression.\n",
    "    If int, then consider min_samples_leaf as the minimum number.\n",
    "    If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples \n",
    "        for each node.\n",
    "min_weight_fraction_leaf = The minimum weighted fraction of the sum total of weights (of all the input samples) \n",
    "    required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
    "max_features{“auto”, “sqrt”, “log2”} = The number of features to consider when looking for the best split:\n",
    "    If int, then consider max_features features at each split.\n",
    "    If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split.\n",
    "    If “auto”, then max_features=sqrt(n_features).\n",
    "    If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).\n",
    "    If “log2”, then max_features=log2(n_features).\n",
    "    If None, then max_features=n_features.\n",
    "max_leaf_nodes = Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity.\n",
    "    If None then unlimited number of leaf nodes.\n",
    "min_impurity_decrease = A node will be split if this split induces a decrease of the impurity greater than or equal to \n",
    "    this value.\n",
    "bootstrap = Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.\n",
    "oob_score = Whether to use out-of-bag samples to estimate the generalization accuracy.\n",
    "n_jobs = The number of jobs to run in parallel. fit, predict, decision_path and apply are all parallelized over the trees.\n",
    "    None means 1 unless in a joblib.parallel_backend context. -1 means using all processors.\n",
    "random_state = Controls both the randomness of the bootstrapping of the samples used when building trees (if bootstrap=True)\n",
    "    and the sampling of the features to consider when looking for the best split at each node (if max_features < n_features).\n",
    "verbose = Controls the verbosity when fitting and predicting.\n",
    "warm_start = When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, \n",
    "    otherwise, just fit a whole new forest. See the Glossary.\n",
    "class_weight{“balanced”, “balanced_subsample”} (CLASSIFIER ONLY) = Weights associated with classes in the form {class_label:\n",
    "    weight}. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be \n",
    "    provided in the same order as the columns of y.\n",
    "    Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}].\n",
    "    The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))\n",
    "    The “balanced_subsample” mode is the same as “balanced” except that weights are computed based on the bootstrap sample for every tree grown.\n",
    "    For multi-output, the weights of each column of y will be multiplied.\n",
    "ccp_alpha = Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that\n",
    "    is smaller than ccp_alpha will be chosen. By default, no pruning is performed. See Minimal Cost-Complexity Pruning for\n",
    "    details.\n",
    "max_samples = If bootstrap is True, the number of samples to draw from X to train each base estimator.\n",
    "    If None (default), then draw X.shape[0] samples.\n",
    "    If int, then draw max_samples samples.\n",
    "    If float, then draw max_samples * X.shape[0] samples. Thus, max_samples should be in the interval (0, 1).\n",
    "    \n",
    "Attributes:\n",
    "base_estimator_ = The child estimator template used to create the collection of fitted sub-estimators.\n",
    "estimators_ = The collection of fitted sub-estimators.\n",
    "classes_ (CLASSIFIER ONLY) = The classes labels (single output problem), or a list of arrays of class labels (multi-output problem).\n",
    "n_classes_ (CLASSIFIER ONLY) = The number of classes (single output problem), or a list containing the number of classes for each output (multi-output problem).\n",
    "n_features_ = The number of features when fit is performed.\n",
    "n_outputs_ = The number of outputs when fit is performed.\n",
    "feature_importances_ = The impurity-based feature importances.\n",
    "oob_score_ = Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when oob_score is True.\n",
    "oob_decision_function_ (CLASSIFIER ONLY) = Decision function computed with out-of-bag estimate on the training set. If n_estimators is small it might be possible that a data point was never left out during the bootstrap. In this case, oob_decision_function_ might contain NaN. This attribute exists only when oob_score is True.\n",
    "oob_prediction_ (REGRESSOR ONLY) = Prediction computed with out-of-bag estimate on the training set. This attribute exists only\n",
    "    when oob_score is True.\n",
    "\n",
    "Methods:\n",
    "apply(X) = Apply trees in the forest to X, return leaf indices.\n",
    "decision_path(X) = Return the decision path in the forest.\n",
    "fit(X, y[, sample_weight]) = Build a forest of trees from the training set (X, y).\n",
    "get_params([deep]) = Get parameters for this estimator.\n",
    "predict(X) = Predict class for X.\n",
    "predict_log_proba(X) (CLASSIFIER ONLY) = Predict class log-probabilities for X.\n",
    "predict_proba(X) (CLASSIFIER ONLY) = Predict class probabilities for X.\n",
    "score(X, y[, sample_weight]) = Return the mean accuracy on the given test data and labels.\n",
    "set_params(**params) = Set the parameters of this estimator.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part I complete\n"
     ]
    }
   ],
   "source": [
    "print('Part I complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
